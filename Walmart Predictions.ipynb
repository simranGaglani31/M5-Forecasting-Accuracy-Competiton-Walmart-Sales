{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "1jX6PtCt9T-O",
        "outputId": "6bd247c5-aedb-4874-d75a-cead962247fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage of dataframe is 0.21 MB\n",
            "Memory usage after optimization is: 0.19 MB\n",
            "Decreased by 8.7%\n",
            "Memory usage of dataframe is 208.77 MB\n",
            "Memory usage after optimization is: 45.76 MB\n",
            "Decreased by 78.1%\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112362 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 381\n",
            "[LightGBM] [Info] Number of data points in the train set: 1920870, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score 1.397507\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\ttraining's rmse: 1.65232\tvalid_1's rmse: 1.63405\n",
            "[200]\ttraining's rmse: 0.770256\tvalid_1's rmse: 0.754558\n",
            "[300]\ttraining's rmse: 0.386154\tvalid_1's rmse: 0.367883\n",
            "[400]\ttraining's rmse: 0.228283\tvalid_1's rmse: 0.204307\n",
            "[500]\ttraining's rmse: 0.168839\tvalid_1's rmse: 0.139591\n",
            "[600]\ttraining's rmse: 0.14554\tvalid_1's rmse: 0.113701\n",
            "[700]\ttraining's rmse: 0.133876\tvalid_1's rmse: 0.101889\n",
            "[800]\ttraining's rmse: 0.127051\tvalid_1's rmse: 0.0953799\n",
            "[900]\ttraining's rmse: 0.123158\tvalid_1's rmse: 0.0901998\n",
            "[1000]\ttraining's rmse: 0.120581\tvalid_1's rmse: 0.0882484\n",
            "[1100]\ttraining's rmse: 0.118487\tvalid_1's rmse: 0.0865748\n",
            "[1200]\ttraining's rmse: 0.117213\tvalid_1's rmse: 0.0840059\n",
            "Early stopping, best iteration is:\n",
            "[1179]\ttraining's rmse: 0.117542\tvalid_1's rmse: 0.0836551\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bfd260ee-d23b-4351-8648-dd4bec418220\", \"sample_submission.csv\", 2635602)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# importing all necessary libraries\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "import plotly.graph_objs as go \n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX \n",
        "from datetime import datetime, timedelta\n",
        "import statsmodels.api as sm\n",
        "import gc\n",
        "from pylab import rcParams\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from xgboost import XGBRegressor\n",
        "import multiprocessing as mp\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# # upload data\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# !unzip WP.zip\n",
        "\n",
        "#save memory \n",
        "# credit to CHANDRIMA D on kaggle - referencing this memory function\n",
        "import numpy as np\n",
        "def reduce_mem_usage(df):\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))   \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype       \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))    \n",
        "    return df\n",
        "\n",
        "# read the data \n",
        "# calendar\n",
        "calendar = pd.read_csv(\"calendar.csv\")\n",
        "#reduce mem calendar\n",
        "reduce_mem_usage(calendar)\n",
        "# sales eval\n",
        "salesT_Eval = pd.read_csv(\"sales_train_evaluation.csv\")\n",
        "salesT_Val = pd.read_csv(\"sales_train_validation.csv\")\n",
        "# prices \n",
        "prices = pd.read_csv(\"sell_prices.csv\")\n",
        "#reduce mem prices \n",
        "reduce_mem_usage(prices)\n",
        "# Format calendar data \n",
        "hols = ['NewYear', 'OrthodoxChristmas', 'MartinLutherKingDay', 'SuperBowl', 'PresidentsDay', 'StPatricksDay', 'Easter', 'Cinco De Mayo', 'IndependenceDay', 'EidAlAdha', 'Thanksgiving', 'Christmas']\n",
        "w = ['Saturday', 'Sunday']\n",
        "r = []\n",
        "# create two functions for feature neineering to pick up how holidays and weekends impact predictions\n",
        "def holidayz(x):\n",
        "    if x in hols:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "dash = [2,3,4] \n",
        "dash2 = [2,3,4]\n",
        "dash3 = [2,3,4]\n",
        "\n",
        "def weekendz(x):\n",
        "    if x in w:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "#Feature engineering to connect nuances \n",
        "calendar['hol1'] = calendar['event_name_1'].apply(holidayz)\n",
        "calendar['hol2'] = calendar['event_name_2'].apply(holidayz)\n",
        "calendar['holidayz'] = calendar[['hol1','hol2']].max(axis=1)\n",
        "calendar['weekendz'] = calendar['weekday'].apply(weekendz)\n",
        "calendar = calendar.drop(['weekday', 'wday', 'month', 'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'], axis='columns')\n",
        "del_col = []\n",
        "for x in range(1851):\n",
        "    del_col.append('d_' + str(x+1))\n",
        "# Train sales eval \n",
        "trainingData = salesT_Eval.drop(del_col, axis='columns')\n",
        "# long version to melt\n",
        "trainingData = trainingData.melt(['id','item_id','dept_id','cat_id','store_id','state_id'], var_name='d', value_name='qty')\n",
        "# merge train sales (elvaualtion) data and calendar \n",
        "trainingData = pd.merge(trainingData, calendar, how='left', on='d')\n",
        "# Merge train sales and prices. (total = eval, calendar, prices)\n",
        "trainingData = pd.merge(trainingData, prices, how='left', on=['item_id', 'wm_yr_wk', 'store_id'])\n",
        "trainingDataTest = trainingData.query('d == \"d_1852\"')\n",
        "trainingDataTest = trainingDataTest[['id', 'store_id', 'item_id', 'dept_id', 'cat_id', 'state_id', 'd', 'qty', 'sell_price']]\n",
        "trainingDataTest['qty'] =trainingDataTest['d'].apply(lambda x: int(x.replace(x, '0')))\n",
        "tempo =trainingDataTest\n",
        "for x in range(28):\n",
        "    trainingDataTest =trainingDataTest.append(tempo)\n",
        "trainingDataTest =trainingDataTest.reset_index(drop=True)\n",
        "theList = []\n",
        "i = 0\n",
        "lst_index = trainingDataTest.index\n",
        "for x in lst_index:\n",
        "    theList.append('d_' + str(((lst_index[i]) // 30490) + 1942))\n",
        "    i = i + 1\n",
        "# list\n",
        "trainingDataTest['d'] = theList\n",
        "# merge eval price and calendar again\n",
        "trainingDataTest = pd.merge(trainingDataTest,calendar, how='left', on='d')\n",
        "submit = pd.read_csv(\"sample_submission.csv\")\n",
        "trainingDataTest = pd.merge(trainingDataTest, prices ,how='left', on=['item_id', 'wm_yr_wk', 'store_id'])\n",
        "# collect trash\n",
        "import gc\n",
        "del tempo\n",
        "gc.collect()\n",
        "#get dummies \n",
        "trainingData = pd.get_dummies(data=trainingData, columns=['dept_id', 'cat_id', 'store_id', 'state_id'])\n",
        "# train = np.mean(trainingData)\n",
        "# dummies for testing\n",
        "trainingDataTest = pd.get_dummies(data=trainingDataTest, columns=['dept_id', 'cat_id', 'store_id', 'state_id'])\n",
        "# format\n",
        "trainingDataTest =trainingDataTest.drop(['sell_price_x', 'snap_CA', 'snap_TX', 'snap_WI'], axis='columns')\n",
        "trainingDataTest = trainingDataTest.rename(columns={'sell_price_y': 'sell_price'})\n",
        "# drop unneded columnns \n",
        "trainingData = trainingData.drop(['snap_CA', 'snap_TX', 'snap_WI'], axis='columns')  \n",
        "# the model \n",
        "from sklearn.model_selection import train_test_split\n",
        "target = 'qty'\n",
        "length = len(target)\n",
        "drCol = ['id', 'item_id', 'd', 'date', 'wm_yr_wk']\n",
        "# keep u array incase needed later \n",
        "u = []\n",
        "featCol = [col for col in trainingData.columns if col not in drCol]\n",
        "y = np.array(trainingData[target])\n",
        "# create x,y\n",
        "X = np.array(trainingData[featCol])\n",
        "X_train, X_test, y_train, y_test = \\\n",
        " train_test_split(X, y, test_size=0.3, random_state=1234)\n",
        "# Run the model \n",
        "model_train = lgb.Dataset(X_train, y_train)\n",
        "# model for both \n",
        "model_eval = lgb.Dataset(X_test, y_test)\n",
        "# tweedie, poisson were tried, but rsme was not increasing\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',  \n",
        "    'boosting_type': 'gbdt',\n",
        "    'bagging_fraction': 0.80,\n",
        "    'bagging_freq': 11, \n",
        "    'n_jobs': -1,\n",
        "    'seed': 236,\n",
        "    'learning_rate': 0.01,\n",
        "    'bagging_fraction': 0.80,\n",
        "    'bagging_freq': 11, \n",
        "    'colsample_bytree': 0.80}\n",
        "model1 = lgb.train(params, model_train, num_boost_round=2000, early_stopping_rounds=50, valid_sets = [model_train, model_eval], verbose_eval=100)\n",
        "# predict \n",
        "pred = model1.predict(trainingDataTest[featCol])\n",
        "counter = 0 \n",
        "trainingDataTest['pred_qty'] = pred\n",
        "pred = trainingDataTest[['id', 'date', 'pred_qty']]\n",
        "# predict\n",
        "pred = pd.pivot(pred, index = 'id', columns = 'date', values = 'pred_qty').reset_index()\n",
        "# predictions\n",
        "# keep track\n",
        "counter = 2 \n",
        "pred = pred.drop(pred.columns[1], axis=1)\n",
        "# predictions\n",
        "pred.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
        "x = 2744099 + 1 - 853720\n",
        "g = trainingData[x:]\n",
        "sub2 = g.copy()\n",
        "finalPred = g[['id', 'date', 'qty']]\n",
        "# open temporary array \n",
        "final = []\n",
        "finalPred = pd.pivot(finalPred, index = 'id', columns = 'date', values = 'qty').reset_index()\n",
        "counter = 3\n",
        "finalPred['id'] = pred['id'].apply(lambda x: x.replace('evaluation', 'validation'))\n",
        "f = len(dash)\n",
        "finalPred.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
        "#finalPred  \n",
        "filename = 'sample_submission.csv'\n",
        "finalPred.to_csv(filename, index=False)\n",
        "from google.colab import files\n",
        "files.download(filename)"
      ]
    }
  ]
}